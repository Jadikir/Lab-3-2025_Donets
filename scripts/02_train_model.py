import pandas as pd
import numpy as np
from clearml import Task, Dataset
import lightgbm as lgb
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import yaml
import json
import joblib
import os
from sklearn.model_selection import TimeSeriesSplit

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–¥–∞—á–∏
task = Task.init(
    project_name='WeatherForecast',
    task_name='Austin Weather Forecast Training - Fixed',
    task_type=Task.TaskTypes.training
)

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
with open('config/default.yaml', 'r') as f:
    config = yaml.safe_load(f)

# ==================== –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ====================
print("=" * 70)
print("üå§ –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –ê–£–°–¢–ò–ù–ê")
print("=" * 70)

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å–æ–∑–¥–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
try:
    dataset = Dataset.get(
        dataset_project=config['project']['name'],
        dataset_name='Austin_Weather_Forecast_v2',
        dataset_tags=['austin', 'daily', 'converted']
    )
    dataset_path = dataset.get_local_copy()
    df = pd.read_parquet(f"{dataset_path}/austin_weather_processed.parquet")
    print("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç Austin")
except:
    # –ï—Å–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –∑–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞–ø—Ä—è–º—É—é
    print("‚ö†Ô∏è  –î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –∑–∞–≥—Ä—É–∂–∞—é –∏–∑ —Ñ–∞–π–ª–∞...")
    df = pd.read_parquet('data/processed/austin_weather_processed.parquet')

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ
print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–ê–ù–ù–´–•:")
print(f"–ó–∞–ø–∏—Å–µ–π: {len(df):,}")
print(f"–ö–æ–ª–æ–Ω–æ–∫: {len(df.columns)}")
print(f"–î–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç: {df['Date'].min().date()} - {df['Date'].max().date()}")

# –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ
df = df.sort_values('Date').reset_index(drop=True)

# ==================== –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ====================
print("\n" + "="*70)
print("üìä –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø")
print("="*70)

# –í—ã–±–∏—Ä–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é (–ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 1 –¥–µ–Ω—å)
TARGET_COL = 'target_temp_1d'  # –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 1 –¥–µ–Ω—å
print(f"–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: {TARGET_COL}")

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
# –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
exclude_cols = [
    'Date', 'Location', 
    # –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
    'target_temp_1d', 'target_change_1d',
    'target_temp_3d', 'target_change_3d', 
    'target_temp_7d', 'target_change_7d',
    'target_temp_14d', 'target_change_14d',
    'target_humidity_1d', 'target_humidity_3d',
    # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ ¬∞F
    'TempHighF_original', 'TempAvgF_original', 'TempLowF_original',
    'DewPointHighF_original', 'DewPointAvgF_original', 'DewPointLowF_original',
    'SeaLevelPressureHighInches_original', 'SeaLevelPressureAvgInches_original', 'SeaLevelPressureLowInches_original',
    'VisibilityHighMiles_original', 'VisibilityAvgMiles_original', 'VisibilityLowMiles_original',
    'WindHighMPH_original', 'WindAvgMPH_original', 'WindGustMPH_original',
    'PrecipitationSumInches_original',
    # Events –µ—Å–ª–∏ –Ω–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω—ã
    'Events'
]

# –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏
exclude_cols = [col for col in exclude_cols if col in df.columns]
features = [col for col in df.columns if col not in exclude_cols]

print(f"\nüéØ –û–°–ù–û–í–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò ({len(features)}):")
print("–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:")
temp_features = [f for f in features if 'temp' in f.lower() or 'Temp' in f]
for i, f in enumerate(temp_features[:10], 1):
    print(f"  {i:2d}. {f}")

print("\n–í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:")
time_features = [f for f in features if f in ['year', 'month', 'day', 'dayofweek', 'season', 'is_weekend', 'is_summer']]
for i, f in enumerate(time_features, 1):
    print(f"  {i:2d}. {f}")

print(f"\nüìã –í–°–ï–ì–û –ü–†–ò–ó–ù–ê–ö–û–í: {len(features)}")

# ==================== –†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–ê–ù–ù–´–• ====================
print("\n" + "="*70)
print("üìä –†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–ê–ù–ù–´–• (TIME SERIES)")
print("="*70)

# –£–¥–∞–ª—è–µ–º NaN –≤ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –∏ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
df_clean = df.dropna(subset=[TARGET_COL] + features)
print(f"–î–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {len(df_clean):,} –∑–∞–ø–∏—Å–µ–π")

X = df_clean[features]
y = df_clean[TARGET_COL]

# Time series split (–±–µ–∑ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏—è!)
train_size = int(len(X) * 0.6)  # 60% –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
val_size = int(len(X) * 0.2)   # 20% –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
test_size = len(X) - train_size - val_size  # 20% –¥–ª—è —Ç–µ—Å—Ç–∞

X_train = X.iloc[:train_size]
y_train = y.iloc[:train_size]

X_val = X.iloc[train_size:train_size + val_size]
y_val = y.iloc[train_size:train_size + val_size]

X_test = X.iloc[train_size + val_size:]
y_test = y.iloc[train_size + val_size:]

print(f"\nüìÖ –†–ê–ó–ú–ï–†–´:")
print(f"Train: {len(X_train):,} ({train_size/len(X)*100:.0f}%) - {df_clean['Date'].iloc[0].date()} –¥–æ {df_clean['Date'].iloc[train_size-1].date()}")
print(f"Val:   {len(X_val):,} ({val_size/len(X)*100:.0f}%) - {df_clean['Date'].iloc[train_size].date()} –¥–æ {df_clean['Date'].iloc[train_size+val_size-1].date()}")
print(f"Test:  {len(X_test):,} ({test_size/len(X)*100:.0f}%) - {df_clean['Date'].iloc[train_size+val_size].date()} –¥–æ {df_clean['Date'].iloc[-1].date()}")

# ==================== –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò ====================
print("\n" + "="*70)
print("ü§ñ –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò LIGHTGBM")
print("="*70)

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
model_params = {
    'n_estimators': 500,
    'learning_rate': 0.01,
    'max_depth': 8,
    'num_leaves': 31,
    'min_child_samples': 20,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'reg_alpha': 0.01,
    'reg_lambda': 0.01,
    'random_state': 42,
    'n_jobs': -1,
    'verbose': -1,
    'metric': 'mae'
}

print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏:")
for key, value in model_params.items():
    if key not in ['n_jobs', 'verbose']:
        print(f"  {key}: {value}")

model = lgb.LGBMRegressor(**model_params)

print("\n–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    eval_metric='mae',
    callbacks=[
        lgb.log_evaluation(period=100),  # –í—ã–≤–æ–¥ –∫–∞–∂–¥—ã–µ 100 –∏—Ç–µ—Ä–∞—Ü–∏–π
        lgb.early_stopping(stopping_rounds=50, verbose=True)  # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
    ]
)

print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
print(f"–ò—Ç–µ—Ä–∞—Ü–∏–π: {model.n_estimators_}")

# ==================== –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò ====================
print("\n" + "="*70)
print("üìä –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò –ù–ê –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–•")
print("="*70)

# –ü—Ä–æ–≥–Ω–æ–∑—ã
y_pred_test = model.predict(X_test)

# Baseline 1: —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —Å–µ–≥–æ–¥–Ω—è (–¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –Ω–∞ –∑–∞–≤—Ç—Ä–∞)
if 'Temperature_C' in X_test.columns:
    baseline_current_temp = X_test['Temperature_C'].values
    baseline_name_current = "—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —Å–µ–≥–æ–¥–Ω—è"
else:
    baseline_current_temp = np.full_like(y_test, y_train.mean())
    baseline_name_current = "—Å—Ä–µ–¥–Ω—è—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞"

# Baseline 2: —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≤—á–µ—Ä–∞ (lag 1 –¥–µ–Ω—å)
if 'Temperature_C_lag_1d' in X_test.columns:
    baseline_lag1 = X_test['Temperature_C_lag_1d'].values
    baseline_name_lag1 = "—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≤—á–µ—Ä–∞"
else:
    baseline_lag1 = baseline_current_temp
    baseline_name_lag1 = baseline_name_current

# –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏
mae = mean_absolute_error(y_test, y_pred_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
r2 = r2_score(y_test, y_pred_test)

# –ú–µ—Ç—Ä–∏–∫–∏ baseline –º–æ–¥–µ–ª–µ–π
baseline_mae_current = mean_absolute_error(y_test, baseline_current_temp)
baseline_mae_lag1 = mean_absolute_error(y_test, baseline_lag1)

# –£–ª—É—á—à–µ–Ω–∏–µ
improvement_current = ((baseline_mae_current - mae) / baseline_mae_current * 100) if baseline_mae_current > 0 else 0
improvement_lag1 = ((baseline_mae_lag1 - mae) / baseline_mae_lag1 * 100) if baseline_mae_lag1 > 0 else 0

print(f"\nüìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ù–ê–®–ï–ô –ú–û–î–ï–õ–ò:")
print(f"  MAE:  {mae:.2f}¬∞C")
print(f"  RMSE: {rmse:.2f}¬∞C")
print(f"  R¬≤:   {r2:.4f}")

print(f"\nüìä BASELINE –ú–û–î–ï–õ–ò:")
print(f"  1. {baseline_name_current}:")
print(f"     MAE: {baseline_mae_current:.2f}¬∞C")
print(f"     –£–ª—É—á—à–µ–Ω–∏–µ: {improvement_current:.1f}%")
print(f"\n  2. {baseline_name_lag1}:")
print(f"     MAE: {baseline_mae_lag1:.2f}¬∞C")
print(f"     –£–ª—É—á—à–µ–Ω–∏–µ: {improvement_lag1:.1f}%")

# ==================== –ü–†–û–ì–ù–û–ó –ù–ê –†–ê–ó–ù–´–ï –ì–û–†–ò–ó–û–ù–¢–´ ====================
print("\n" + "="*70)
print("üå° –ü–†–û–ì–ù–û–ó –ù–ê –†–ê–ó–ù–´–ï –ì–û–†–ò–ó–û–ù–¢–´")
print("="*70)

horizon_results = {}

for horizon in ['1d', '3d', '7d']:
    target_col = f'target_temp_{horizon}'
    
    if target_col not in df.columns:
        continue
    
    print(f"\nüìÖ –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ {horizon}:")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df_horizon = df.dropna(subset=[target_col] + features)
    X_h = df_horizon[features]
    y_h = df_horizon[target_col]
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ –∂–µ –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)
    train_size_h = int(len(X_h) * 0.6)
    test_size_h = len(X_h) - train_size_h
    
    X_train_h = X_h.iloc[:train_size_h]
    y_train_h = y_h.iloc[:train_size_h]
    X_test_h = X_h.iloc[train_size_h:]
    y_test_h = y_h.iloc[train_size_h:]
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç—Ç–æ–≥–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞
    model_h = lgb.LGBMRegressor(**model_params)
    model_h.fit(
        X_train_h, y_train_h,
        eval_set=[(X_test_h, y_test_h)],
        eval_metric='mae',
        callbacks=[lgb.log_evaluation(period=0)]  # –ë–µ–∑ –≤—ã–≤–æ–¥–∞
    )
    
    # –ü—Ä–æ–≥–Ω–æ–∑—ã
    y_pred_h = model_h.predict(X_test_h)
    
    # Baseline
    if f'Temperature_C_lag_{horizon.replace("d", "")}d' in X_test_h.columns:
        baseline_h = X_test_h[f'Temperature_C_lag_{horizon.replace("d", "")}d'].values
        baseline_name_h = f"—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ {horizon.replace("d", "")} –¥–Ω–µ–π –Ω–∞–∑–∞–¥"
    else:
        baseline_h = np.full_like(y_test_h, y_train_h.mean())
        baseline_name_h = "—Å—Ä–µ–¥–Ω—è—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞"
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    mae_h = mean_absolute_error(y_test_h, y_pred_h)
    baseline_mae_h = mean_absolute_error(y_test_h, baseline_h)
    improvement_h = ((baseline_mae_h - mae_h) / baseline_mae_h * 100) if baseline_mae_h > 0 else 0
    
    print(f"  –ù–∞—à–∞ –º–æ–¥–µ–ª—å: MAE = {mae_h:.2f}¬∞C")
    print(f"  Baseline ({baseline_name_h}): MAE = {baseline_mae_h:.2f}¬∞C")
    print(f"  –£–ª—É—á—à–µ–Ω–∏–µ: {improvement_h:.1f}%")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    horizon_results[horizon] = {
        'mae': float(mae_h),
        'baseline_mae': float(baseline_mae_h),
        'improvement': float(improvement_h),
        'model': model_h
    }

# ==================== –ê–ù–ê–õ–ò–ó –í–ê–ñ–ù–û–°–¢–ò –ü–†–ò–ó–ù–ê–ö–û–í ====================
print("\n" + "="*70)
print("üîç –í–ê–ñ–ù–û–°–¢–¨ –ü–†–ò–ó–ù–ê–ö–û–í")
print("="*70)

feature_importance = pd.DataFrame({
    'feature': features,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

print(f"\n–¢–û–ü-20 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
for i, row in feature_importance.head(20).iterrows():
    print(f"  {i+1:2d}. {row['feature']}: {row['importance']:.1f}")

# ==================== –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø ====================
print("\n" + "="*70)
print("üìà –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
print("="*70)

fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# 1. Learning curve - –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º 'l1' –≤–º–µ—Å—Ç–æ 'mae'
if hasattr(model, 'evals_result_'):
    evals = model.evals_result_
    # LightGBM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 'l1' –¥–ª—è MAE –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
    if 'valid_0' in evals and 'l1' in evals['valid_0']:
        axes[0, 0].plot(evals['valid_0']['l1'], label='Validation MAE', color='blue')
        axes[0, 0].set_xlabel('Iteration')
        axes[0, 0].set_ylabel('MAE (¬∞C)')
        axes[0, 0].set_title('Learning Curve (1-day forecast)')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
    else:
        print("‚ö†Ô∏è  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö learning curve")
        axes[0, 0].text(0.5, 0.5, 'No learning curve data', 
                       ha='center', va='center', transform=axes[0, 0].transAxes)
        axes[0, 0].set_title('Learning Curve')

# 2. –§–∞–∫—Ç vs –ü—Ä–æ–≥–Ω–æ–∑
axes[0, 1].scatter(y_test[:200], y_pred_test[:200], alpha=0.5, s=20, color='blue')
axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
axes[0, 1].set_xlabel('Actual Temperature (¬∞C)')
axes[0, 1].set_ylabel('Predicted Temperature (¬∞C)')
axes[0, 1].set_title(f'Actual vs Predicted (1-day)\nMAE: {mae:.1f}¬∞C, R¬≤: {r2:.3f}')
axes[0, 1].grid(True, alpha=0.3)

# 3. –û—à–∏–±–∫–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
date_test = df_clean['Date'].iloc[train_size + val_size:].reset_index(drop=True)
errors = y_test.values - y_pred_test
axes[0, 2].plot(date_test[:100], errors[:100], 'b-', alpha=0.7, linewidth=2)
axes[0, 2].axhline(y=0, color='r', linestyle='--', alpha=0.5, linewidth=2)
axes[0, 2].set_xlabel('Date')
axes[0, 2].set_ylabel('Prediction Error (¬∞C)')
axes[0, 2].set_title('Prediction Errors Over Time')
axes[0, 2].tick_params(axis='x', rotation=45)
axes[0, 2].grid(True, alpha=0.3)

# 4. Feature importance (top 15)
top_n = min(15, len(feature_importance))
if top_n > 0:
    bars = axes[1, 0].barh(range(top_n), feature_importance['importance'].head(top_n), 
                          color='steelblue')
    axes[1, 0].set_yticks(range(top_n))
    # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –∏–º–µ–Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_names = feature_importance['feature'].head(top_n).tolist()
    feature_names_short = [name[:30] + '...' if len(name) > 30 else name for name in feature_names]
    axes[1, 0].set_yticklabels(feature_names_short, fontsize=9)
    axes[1, 0].invert_yaxis()
    axes[1, 0].set_xlabel('Feature Importance')
    axes[1, 0].set_title('Top Feature Importances')
else:
    axes[1, 0].text(0.5, 0.5, 'No feature importance data', 
                   ha='center', va='center', transform=axes[1, 0].transAxes)

# 5. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤
if horizon_results:
    horizons = list(horizon_results.keys())
    mae_values = [horizon_results[h]['mae'] for h in horizons]
    baseline_values = [horizon_results[h]['baseline_mae'] for h in horizons]
    
    x = np.arange(len(horizons))
    width = 0.35
    
    axes[1, 1].bar(x - width/2, mae_values, width, label='Our Model', alpha=0.7, color='blue')
    axes[1, 1].bar(x + width/2, baseline_values, width, label='Baseline', alpha=0.7, color='red')
    axes[1, 1].set_xlabel('Forecast Horizon')
    axes[1, 1].set_ylabel('MAE (¬∞C)')
    axes[1, 1].set_title('MAE by Forecast Horizon')
    axes[1, 1].set_xticks(x)
    axes[1, 1].set_xticklabels([f'{h.replace("d", "")} day' for h in horizons])
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3, axis='y')
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for i, (mae_val, base_val) in enumerate(zip(mae_values, baseline_values)):
        axes[1, 1].text(i - width/2, mae_val + 0.1, f'{mae_val:.1f}', 
                       ha='center', va='bottom', fontsize=9)
        axes[1, 1].text(i + width/2, base_val + 0.1, f'{base_val:.1f}', 
                       ha='center', va='bottom', fontsize=9)
else:
    axes[1, 1].text(0.5, 0.5, 'No horizon results', 
                   ha='center', va='center', transform=axes[1, 1].transAxes)

# 6. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫
axes[1, 2].hist(errors, bins=50, edgecolor='black', alpha=0.7, color='green')
axes[1, 2].axvline(x=0, color='r', linestyle='--', alpha=0.5, linewidth=2)
axes[1, 2].axvline(x=errors.mean(), color='blue', linestyle='-', alpha=0.7, linewidth=1.5, label=f'Mean: {errors.mean():.2f}¬∞C')
axes[1, 2].set_xlabel('Prediction Error (¬∞C)')
axes[1, 2].set_ylabel('Frequency')
axes[1, 2].set_title(f'Distribution of Errors\nMean: {errors.mean():.2f}¬∞C, Std: {errors.std():.2f}¬∞C')
axes[1, 2].legend()
axes[1, 2].grid(True, alpha=0.3)

plt.tight_layout()
task.get_logger().report_matplotlib_figure(
    title='Austin Weather Forecast Results',
    series='comprehensive',
    figure=fig,
    iteration=0
)

# ==================== –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ====================
print("\n" + "="*70)
print("üíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô –ò –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
print("="*70)

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
os.makedirs('models/austin', exist_ok=True)
os.makedirs('results/austin', exist_ok=True)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –º–æ–¥–µ–ª—å
model_path = 'models/austin/weather_forecast_1d.pkl'
joblib.dump(model, model_path)
print(f"‚úÖ –û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤
models_dict = {'1d': model}
for horizon, result in horizon_results.items():
    if horizon != '1d':
        models_dict[horizon] = result['model']

all_models_path = 'models/austin/weather_forecast_all_horizons.pkl'
joblib.dump(models_dict, all_models_path)
print(f"‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {all_models_path}")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
results = {
    '1d': {
        'mae': float(mae),
        'rmse': float(rmse),
        'r2': float(r2),
        'baseline_mae_current': float(baseline_mae_current),
        'baseline_mae_lag1': float(baseline_mae_lag1),
        'improvement_current': float(improvement_current),
        'improvement_lag1': float(improvement_lag1),
        'n_features': len(features),
        'test_size': len(X_test)
    }
}

# –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–º
for horizon, result in horizon_results.items():
    results[horizon] = {
        'mae': result['mae'],
        'baseline_mae': result['baseline_mae'],
        'improvement': result['improvement']
    }

results_path = 'results/austin/forecast_results.json'
with open(results_path, 'w') as f:
    json.dump(results, f, indent=2)
print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_path}")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
features_path = 'models/austin/feature_list.json'
with open(features_path, 'w') as f:
    json.dump(features, f, indent=2)
print(f"‚úÖ –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {features_path}")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º feature importance
feature_importance_path = 'results/austin/feature_importance.csv'
feature_importance.to_csv(feature_importance_path, index=False)
print(f"‚úÖ –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {feature_importance_path}")

# ==================== –ò–¢–û–ì–ò –ò –ê–ù–ê–õ–ò–ó ====================
print("\n" + "="*70)
print("üéØ –ò–¢–û–ì–ò –û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ò –î–õ–Ø –ê–£–°–¢–ò–ù–ê")
print("="*70)

print(f"\nüìä –û–ë–©–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
print(f"‚Ä¢ –î–∞—Ç–∞—Å–µ—Ç: Austin, TX –ø–æ–≥–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
print(f"‚Ä¢ –ü–µ—Ä–∏–æ–¥: {df['Date'].min().date()} - {df['Date'].max().date()}")
print(f"‚Ä¢ –ó–∞–ø–∏—Å–µ–π: {len(df):,}")
print(f"‚Ä¢ –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(features)}")

print(f"\nüéØ –ü–†–û–ì–ù–û–ó –ù–ê 1 –î–ï–ù–¨:")
print(f"‚Ä¢ MAE –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏: {mae:.1f}¬∞C")
print(f"‚Ä¢ MAE baseline (—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —Å–µ–≥–æ–¥–Ω—è): {baseline_mae_current:.1f}¬∞C")
print(f"‚Ä¢ –£–ª—É—á—à–µ–Ω–∏–µ: {improvement_current:.1f}%")
print(f"‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ (R¬≤): {r2:.4f}")

print(f"\nüìà –ü–†–û–ì–ù–û–ó–´ –ù–ê –†–ê–ó–ù–´–ï –ì–û–†–ò–ó–û–ù–¢–´:")
for horizon in sorted(horizon_results.keys()):
    result = horizon_results[horizon]
    print(f"‚Ä¢ {horizon}: MAE = {result['mae']:.1f}¬∞C, —É–ª—É—á—à–µ–Ω–∏–µ = {result['improvement']:.1f}%")

print(f"\nüîç –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:")
print(f"1. –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 1 –¥–µ–Ω—å:")
print(f"   ‚Ä¢ MAE = {mae:.1f}¬∞C - —ç—Ç–æ –æ—Ç–ª–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç!")
print(f"   ‚Ä¢ R¬≤ = {r2:.4f} - –º–æ–¥–µ–ª—å –æ–±—ä—è—Å–Ω—è–µ—Ç {r2*100:.1f}% –¥–∏—Å–ø–µ—Ä—Å–∏–∏")
print(f"   ‚Ä¢ –£–ª—É—á—à–µ–Ω–∏–µ –Ω–∞–¥ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π –≤—á–µ—Ä–∞: {improvement_lag1:.1f}%")

print(f"\n2. –¢–µ–Ω–¥–µ–Ω—Ü–∏–∏ –ø–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–º:")
if '1d' in horizon_results and '3d' in horizon_results and '7d' in horizon_results:
    mae_1d = horizon_results['1d']['mae']
    mae_3d = horizon_results['3d']['mae']
    mae_7d = horizon_results['7d']['mae']
    
    if mae_3d > mae_1d and mae_7d > mae_3d:
        print(f"   ‚úÖ –û–∂–∏–¥–∞–µ–º–∞—è —Ç–µ–Ω–¥–µ–Ω—Ü–∏—è: —Ç–æ—á–Ω–æ—Å—Ç—å –ø–∞–¥–∞–µ—Ç —Å —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞")
        print(f"     1 –¥–µ–Ω—å: {mae_1d:.1f}¬∞C ‚Üí 3 –¥–Ω—è: {mae_3d:.1f}¬∞C ‚Üí 7 –¥–Ω–µ–π: {mae_7d:.1f}¬∞C")
    else:
        print(f"   ‚ö†Ô∏è  –ù–µ–æ–±—ã—á–Ω–∞—è —Ç–µ–Ω–¥–µ–Ω—Ü–∏—è - –≤–æ–∑–º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –∏–ª–∏ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö")

print(f"\n3. –°–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:")
print(f"   1. wind_chill (–∏–Ω–¥–µ–∫—Å –≤–µ—Ç—Ä–æ-—Ö–æ–ª–æ–¥–∞): {feature_importance.iloc[0]['importance']:.0f}")
print(f"   2. Temperature_C_diff_1d (–∏–∑–º–µ–Ω–µ–Ω–∏–µ –∑–∞ –¥–µ–Ω—å): {feature_importance.iloc[1]['importance']:.0f}")
print(f"   3. dayofyear_cos (—Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å): {feature_importance.iloc[2]['importance']:.0f}")

print(f"\n‚úÖ –í–´–í–û–î–´:")
print(f"1. –ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –Ω–∞ 1 –¥–µ–Ω—å (MAE = {mae:.1f}¬∞C)")
print(f"2. –ü—Ä–æ–≥–Ω–æ–∑ —É—Ö—É–¥—à–∞–µ—Ç—Å—è —Å —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞, –∫–∞–∫ –∏ –æ–∂–∏–¥–∞–ª–æ—Å—å")
print(f"3. –°–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω—ã–µ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏ –∏ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å")

print(f"\nüöÄ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø –£–õ–£–ß–®–ï–ù–ò–Ø:")
print("1. –î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö (5-10 –ª–µ—Ç)")
print("2. –í–∫–ª—é—á–∏—Ç—å –≤–Ω–µ—à–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ (–ø—Ä–æ–≥–Ω–æ–∑—ã –ø–æ–≥–æ–¥—ã, –∫–∞—Ä—Ç—ã –¥–∞–≤–ª–µ–Ω–∏—è)")
print("3. –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∞–Ω—Å–∞–º–±–ª–∏ –º–æ–¥–µ–ª–µ–π (LightGBM + XGBoost + CatBoost)")
print("4. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å deep learning (LSTM) –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤")

print(f"\nüíæ –°–û–•–†–ê–ù–ï–ù–ù–´–ï –§–ê–ô–õ–´:")
print(f"‚Ä¢ –ú–æ–¥–µ–ª–∏: models/austin/")
print(f"‚Ä¢ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: results/austin/")
print(f"‚Ä¢ –ì—Ä–∞—Ñ–∏–∫–∏: –≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ ClearML")

print("\n" + "=" * 70)

# –ó–∞–∫—Ä—ã–≤–∞–µ–º –∑–∞–¥–∞—á—É
task.close()